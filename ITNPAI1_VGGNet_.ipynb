{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajjeoor/ITNPAI_Project_Forestcityclassifier/blob/main/ITNPAI1_VGGNet_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#University of Stirling - Spring 2023\n",
        "\n",
        "## ITNPAI1 - Deep Learning for Vision and NLP (2022/3)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4i5afvUbhmGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 1. **Problem definition** \n",
        "There is a need to create reliable and effective techniques to automatically categorise photos given the increasing volume of digital imagery. The richness and unpredictability of image information, as well as the necessity for robustness to image variables like lighting, stance, and occlusion, make image classification a tough task. A crucial area of research, picture categorization is also a critical task in many industries, including robotics, security, and healthcare. Thus, it is necessary to create sophisticated computer vision methods that can identify images reliably and effectively in a variety of fields.\n",
        "\n",
        "One of the most important jobs in computer vision is the assignment of a label or a category to an input image based on the visual content of the image. The following are some examples of how crucial Image classification is to computer vision:\n",
        "\n",
        "Identifying objects in an image and classifying them into distinct classes or categories can aid in object detection. This is helpful in applications like self-driving automobiles where it is necessary to identify and track items like pedestrians, traffic signs, and other vehicles.\n",
        "\n",
        "By classifying photographs according to their content, it is possible to find related pictures in a sizable database. This is helpful in systems like content-based picture retrieval systems and image search engines.\n",
        "\n",
        "In order to complete the objective, a dataset of labelled photos from Accra, Ghana, and Pune, India, will be used as training data to create a computer vision system that can reliably categorise cars in images from that location. The next step is to assess the algorithm's performance using car photos from Stirling, Scotlandâ€”that wasn't a part of the training set. The classification task entails classifying forests from streets ensuring that the system can correctly categorise the same across various cities and environmental situations, such as shifting lighting, weather, and background, will be a problem.\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)\n",
        " "
      ],
      "metadata": {
        "id": "hglJVRRslqMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2. **Dataset creation**\n",
        "\n",
        "The dataset for this solution is already uploaded on the google drive\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "qEgFzxmWrGA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 3. **Dataloader**\n",
        "\n",
        "The Dataloader in pytorch comes up with some image augmentation by default but in order to make the model more robust. We are mannually implementing data augmentation techniques like random flips equalizing the histogram of the image and normalizing the pixels. Here we are using the image dataset function which creates the dataloader by keeping the path of the directory in consideration.\n",
        "\n",
        "[top](scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "EDd6lLwlx4un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib\n",
        "from torchsummary import summary\n",
        "from torch.nn import LeakyReLU\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jJs8HpW_zX0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuoIFwRmhueB",
        "outputId": "f27cb8b4-792b-4554-d0ac-3ba02b9b9951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries for authentication\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "3CAab17oheUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for authentication\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n"
      ],
      "metadata": {
        "id": "RaPd82NmyNCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Image augmentation Compose\n",
        "transforms_train=transforms.Compose([\n",
        "    transforms.Resize((224,224)), #resizing the image according to the numbers of pixels\n",
        "    transforms.RandomHorizontalFlip(0.5),#images will be flipped randomly with the probability of 50%\n",
        "    transforms.ToTensor(),  # converts pixel resolution of 0-255 to 0-1\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) #pixel normalization\n",
        "])\n",
        "\n",
        "transforms_test=transforms.Compose([\n",
        "     transforms.Resize((224,224)),\n",
        "    transforms.RandomEqualize(0.5), #Randomly equalizing the histogram of the image\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "pAFQblcHimNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloader\n",
        "train_path=\"/content/drive/MyDrive/Deep_Learning_Assignment/Classifier_dataset/train\"\n",
        "train_dataloader=DataLoader(torchvision.datasets.ImageFolder(train_path,transforms_train),batch_size=32,shuffle=True)\n",
        "test_path=\"/content/drive/MyDrive/Deep_Learning_Assignment/Classifier_dataset/test\"\n",
        "test_dataloader=DataLoader(torchvision.datasets.ImageFolder(test_path,transforms_test),batch_size=32,shuffle=True)"
      ],
      "metadata": {
        "id": "4NizoWiZiq8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 4. **Proposed solution** \n",
        "\n",
        "In this particular solution, we have used VGG nets. VGG Network can be partitioned into two parts: the first consisting mostly of convolutional and pooling layers and the second consisting of fully connected layer\n",
        "\n",
        "In this solution, first we implemented convolution layer that will accept input images of resolution 224X224 pixels.Then the batch normalization has been performed, then the output of batch normalization has been passed through the \n",
        "RelU function and that output is maxpooled. After this, we are looping the vggblocks according to number of blocks and input channels.\n",
        "\n",
        "In each VGG block, We are passing lazy convolution layer. Lazy initialization can be convenient, allowing the framework to infer parameter shapes automatically, making it easy to modify architectures and eliminating one common source of errors.\n",
        "\n",
        "After this block we have implemented ReLU function to introduce the non linearity. \n",
        "\n",
        "After RelU, We implemented the maxpooling layer.\n",
        "\n",
        "After all of this,under the fully connected layer the input layers consists of 2048 nodes, the weights of these nodes passed to the hidden layes having 4096 nodes of which the weights of those nodes passed to another 4096 nodes. From there,all of these weights has been passed to the output layers.\n",
        "\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "ScTrpUW8zOp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device initiallization\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "kus0e07jiTup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device printing\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkc9AgPCifV6",
        "outputId": "488320cc-bee9-49d7-f526-77d89ab43eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the root directory to extract the number of classes\n",
        "root=pathlib.Path(train_path)\n",
        "\n",
        "#Extracting the name of the classes from the assigned root directory, under which the image has to be classified\n",
        "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
      ],
      "metadata": {
        "id": "EWJke4qmi08s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the name of the classes\n",
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR3olz3zjPip",
        "outputId": "6c81f298-842f-448c-d468-c3c5a00d4ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Forests', 'Streets']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the shapes of the dataloader\n",
        "for x,y in test_dataloader:\n",
        "  print(y.shape)\n",
        "  print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jeLliLJjU5W",
        "outputId": "c085a2a2-5581-4beb-cc0a-44bf2d6bfe5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([8])\n",
            "torch.Size([8, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#definition of VGG block function\n",
        "def vggblock(num_conv,out_channels):\n",
        "  layers=[]\n",
        "  for _ in range(num_conv):\n",
        "    layers.append(nn.LazyConv2d(out_channels=out_channels,kernel_size=3,padding=2))\n",
        "    layers.append(nn.ReLU())\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "  return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "6GopTrDGj3PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Convnet(nn.Module):\n",
        "  def __init__(self,arch=[],num_classes=2):\n",
        "    super(Convnet,self).__init__()\n",
        "    #Output size after convolution filter\n",
        "    #(w-f+2P)/s+1\n",
        "\n",
        "    #input shape = (64,3,224,224)\n",
        "    self.convl=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
        "    #Shape=(64,12,224,224)\n",
        "    self.bnl=nn.BatchNorm2d(num_features=12)\n",
        "    #Shape=(64,12,224,224)\n",
        "    self.relu1=nn.ReLU()\n",
        "\n",
        "    self.pool=nn.MaxPool2d(kernel_size=2)\n",
        "    #Reduce the image size by the factor of 2\n",
        "    conv_blks=[]\n",
        "\n",
        "    # Repeating the number of vggblocks according to the architecture described in the next code snippet\n",
        "    for nums_convs,out_channels in arch:\n",
        "      conv_blks.append(vggblock(nums_convs,out_channels))\n",
        "\n",
        "\n",
        "    #merging all the blocks under the Sequential\n",
        "    self.features=nn.Sequential(*conv_blks)  \n",
        "\n",
        "    #From this juncture, Sequential network starts\n",
        "    self.fc=nn.Sequential(\n",
        "            nn.Flatten(),                                                                     # input: (b, 512, 2, 2) e output: (b, 512*2*2) = (b, 2048)\n",
        "            nn.Linear(in_features=2048, out_features=4096),                                    # input: (b, 2048) e output: (b, 4096)\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),                                                             # input: (b, 4096) e output: (b, 4096)\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)                                                          # input: (b, 4096) e output: (b, 2)\n",
        "        )\n",
        "    \n",
        "    #under the forward function we are composing all of the process which defined previously through which the input image will go through\n",
        "  def forward(self,input):\n",
        "      output=self.convl(input)\n",
        "      output=self.bnl(output)\n",
        "      output=self.relu1(output)\n",
        "      output=self.pool(output)\n",
        "      output=self.features(output)\n",
        "      output=self.fc(output)\n",
        "      return output\n"
      ],
      "metadata": {
        "id": "PUkArC2Yj7yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the model with architecture arch\n",
        "#in this block of code arch is the tuple of pairs of numbers under the format (number_of_vggblocks,number of filters)\n",
        "#for example if there is (1,128), 1 will be number of vggblock/'s and 128 will be number of filters\n",
        "model=Convnet(arch=((1, 128), (1, 256), (2, 512), (2, 1024), (2, 512)))\n",
        "\n",
        "summary(model,(3,224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ4hotDXkCoz",
        "outputId": "05ed8868-e208-4fad-c8c3-ef48775524cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 12, 224, 224]             336\n",
            "       BatchNorm2d-2         [-1, 12, 224, 224]              24\n",
            "              ReLU-3         [-1, 12, 224, 224]               0\n",
            "         MaxPool2d-4         [-1, 12, 112, 112]               0\n",
            "            Conv2d-5        [-1, 128, 114, 114]          13,952\n",
            "              ReLU-6        [-1, 128, 114, 114]               0\n",
            "         MaxPool2d-7          [-1, 128, 57, 57]               0\n",
            "            Conv2d-8          [-1, 256, 59, 59]         295,168\n",
            "              ReLU-9          [-1, 256, 59, 59]               0\n",
            "        MaxPool2d-10          [-1, 256, 29, 29]               0\n",
            "           Conv2d-11          [-1, 512, 31, 31]       1,180,160\n",
            "             ReLU-12          [-1, 512, 31, 31]               0\n",
            "        MaxPool2d-13          [-1, 512, 15, 15]               0\n",
            "           Conv2d-14          [-1, 512, 17, 17]       2,359,808\n",
            "             ReLU-15          [-1, 512, 17, 17]               0\n",
            "        MaxPool2d-16            [-1, 512, 8, 8]               0\n",
            "           Conv2d-17         [-1, 1024, 10, 10]       4,719,616\n",
            "             ReLU-18         [-1, 1024, 10, 10]               0\n",
            "        MaxPool2d-19           [-1, 1024, 5, 5]               0\n",
            "           Conv2d-20           [-1, 1024, 7, 7]       9,438,208\n",
            "             ReLU-21           [-1, 1024, 7, 7]               0\n",
            "        MaxPool2d-22           [-1, 1024, 3, 3]               0\n",
            "           Conv2d-23            [-1, 512, 5, 5]       4,719,104\n",
            "             ReLU-24            [-1, 512, 5, 5]               0\n",
            "        MaxPool2d-25            [-1, 512, 2, 2]               0\n",
            "           Conv2d-26            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 512, 2, 2]               0\n",
            "          Flatten-29                 [-1, 2048]               0\n",
            "           Linear-30                 [-1, 4096]       8,392,704\n",
            "             ReLU-31                 [-1, 4096]               0\n",
            "          Dropout-32                 [-1, 4096]               0\n",
            "           Linear-33                 [-1, 4096]      16,781,312\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                    [-1, 2]           8,194\n",
            "================================================================\n",
            "Total params: 50,268,394\n",
            "Trainable params: 50,268,394\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 72.77\n",
            "Params size (MB): 191.76\n",
            "Estimated Total Size (MB): 265.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up the learning rate and weights decay\n",
        "lr=0.1\n",
        "wd_lambda=0.0001\n",
        "\n",
        "#importing optim module from the torch library for the optimizer\n",
        "from torch import optim\n",
        "\n",
        "#Selecting optimizer as stochastic gradient descent\n",
        "optimizer1=optim.SGD(model.parameters(),lr=0.01,weight_decay=0.0001,momentum=0.9)\n",
        "\n",
        "#Setting up the loss function as cross entropy loss function\n",
        "loss_function=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "KylxfghdkhDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up number of epochs and determining the number of train count and test count.\n",
        "num_epochs=20\n",
        "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
        "test_count=len(glob.glob(test_path+'/**/*.jpg'))\n",
        "print(train_count)\n",
        "print(test_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_a3ER79kurB",
        "outputId": "d849f2aa-4521-4d62-e3b7-008ebe0dbb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Model_test_eval(net,optimizer,loss):\n",
        "    best_accuracy=0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      train_acc=0.0\n",
        "      train_loss=0.0\n",
        "      for i,(images,labels) in enumerate(train_dataloader):\n",
        "        #checking if this system utilizing GPU\n",
        "        if torch.cuda.is_available():\n",
        "          images=Variable(images.cuda)\n",
        "          labels=Variable(labels.cuda)\n",
        "        #Initializing the optimizer to zero gradient  \n",
        "        optimizer1.zero_grad()\n",
        "        #Extracting Yhat by inputting images through the model\n",
        "        outputs=net(images)\n",
        "\n",
        "        #calculating loss\n",
        "        loss1=loss(outputs,labels)\n",
        "\n",
        "        #backward propagation\n",
        "        loss1.backward()\n",
        "\n",
        "        #updating the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        #assigning the train loss\n",
        "        train_loss+=loss1.cpu().data*images.size(0)\n",
        "\n",
        "        #predictingthe value\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "\n",
        "        #Calculating the training accuracy and assigning loss function\n",
        "        train_acc+=int(torch.sum(prediction==labels.data))\n",
        "\n",
        "        train_accuracy=train_acc/train_count\n",
        "        train_loss=train_loss/train_count\n",
        "\n",
        "      #Testing the model on unseen ( i.e. testing data )\n",
        "      net.eval()\n",
        "      test_accuracy=0.0\n",
        "      for i,(images,labels) in enumerate(test_dataloader):\n",
        "        if torch.cuda.is_available():\n",
        "          images=Variable(images.cuda)\n",
        "          labels=Variable(labels.cuda)\n",
        "      \n",
        "        outputs=net(images)\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
        "\n",
        "        test_accuracy=test_accuracy/test_count\n",
        "\n",
        "      print('Epoch:'+str(epoch)+' Train Loss :'+str(train_loss)+' Train Accuracy:'+str(train_accuracy)+' Test Accuracy:'+str(test_accuracy)) "
      ],
      "metadata": {
        "id": "q81hz25Gk6UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 5. **Experimental tests and evaluations** \n",
        "\n",
        "  - *E1* and *E2* - Training and testing the models.Here we are implementing both training and testing on the model and functions has been developed from the above code snippet under this function we are passing model, optimizer and the loss function.\n",
        "\n",
        "  -*E3* - \n",
        "By keeping the models accuracy in consideration we did not went for this stage but we went for other solutions that has been provided under the github repository:\n",
        "\n",
        "We tried changing hyper parameters like implementation of Adam optimizer and changing activation functions from RelU to LeakyRelU. But we observed no difference. It appears like due to incompatibility of images with models, the model is overfitting. It forced us to explore another solutions which has been shared under the following link:\n",
        "\n",
        "https://github.com/surajjeoor/ITNPAI_Project_Forestcityclassifier\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "3RBW58of0ZDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and testing models simultaneously\n",
        "\n",
        "Model_test_eval(model,optimizer=optimizer1,loss=loss_function)\n",
        "\n"
      ],
      "metadata": {
        "id": "jHWwdXg32BEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6448c9f-8ca1-42ec-abbf-6197b734df30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 Train Loss :tensor(0.0283) Train Accuracy:0.46 Test Accuracy:0.02535176193151664\n",
            "Epoch:1 Train Loss :tensor(0.0283) Train Accuracy:0.485 Test Accuracy:0.025451760678344922\n",
            "Epoch:2 Train Loss :tensor(0.0280) Train Accuracy:0.5 Test Accuracy:0.03027763504712617\n",
            "Epoch:3 Train Loss :tensor(0.0285) Train Accuracy:0.5 Test Accuracy:0.025376886915860784\n",
            "Epoch:4 Train Loss :tensor(0.0282) Train Accuracy:0.5 Test Accuracy:0.010451386312720079\n",
            "Epoch:5 Train Loss :tensor(0.0286) Train Accuracy:0.49 Test Accuracy:0.02542676194084453\n",
            "Epoch:6 Train Loss :tensor(0.0281) Train Accuracy:0.55 Test Accuracy:0.02035188630029797\n",
            "Epoch:7 Train Loss :tensor(0.0279) Train Accuracy:0.5 Test Accuracy:0.0104025069284075\n",
            "Epoch:8 Train Loss :tensor(0.0279) Train Accuracy:0.5 Test Accuracy:0.020427133809594922\n",
            "Epoch:9 Train Loss :tensor(0.0275) Train Accuracy:0.5 Test Accuracy:0.025327133800251485\n",
            "Epoch:10 Train Loss :tensor(0.0288) Train Accuracy:0.5 Test Accuracy:0.015502008169048048\n",
            "Epoch:11 Train Loss :tensor(0.0290) Train Accuracy:0.5 Test Accuracy:0.030401636287720392\n",
            "Epoch:12 Train Loss :tensor(0.0284) Train Accuracy:0.495 Test Accuracy:0.025352135047141875\n",
            "Epoch:13 Train Loss :tensor(0.0286) Train Accuracy:0.485 Test Accuracy:0.01047663380339203\n",
            "Epoch:14 Train Loss :tensor(0.0283) Train Accuracy:0.5 Test Accuracy:0.02032713755957922\n",
            "Epoch:15 Train Loss :tensor(0.0283) Train Accuracy:0.5 Test Accuracy:0.025452134412782424\n",
            "Epoch:16 Train Loss :tensor(0.0281) Train Accuracy:0.5 Test Accuracy:0.030451760665876248\n",
            "Epoch:17 Train Loss :tensor(0.0284) Train Accuracy:0.5 Test Accuracy:0.010327134428392108\n",
            "Epoch:18 Train Loss :tensor(0.0282) Train Accuracy:0.5 Test Accuracy:0.025377008187751172\n",
            "Epoch:19 Train Loss :tensor(0.0288) Train Accuracy:0.5 Test Accuracy:0.015502260034641797\n"
          ]
        }
      ]
    }
  ]
}